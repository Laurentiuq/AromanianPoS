{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU3HO2eTowWH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ZJS1M4c-oxn4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "from collections import Counter, OrderedDict\n",
        "from dataclasses import dataclass\n",
        "from time import monotonic\n",
        "from typing import Dict, List, Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from scipy.spatial.distance import cosine\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.datasets import WikiText103\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aromanian_iterator(file_path, chunk_size=1024):\n",
        "    \"\"\"\n",
        "    Creates an iterator over the WikiText-103 file.\n",
        "\n",
        "    :param file_path: Path to the WikiText-103 file.\n",
        "    :param chunk_size: Number of characters to read in each iteration.\n",
        "    :return: Yields chunks of text from the file.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        while True:\n",
        "            chunk = file.read(chunk_size)\n",
        "            if not chunk:\n",
        "                break\n",
        "            yield chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cu dragostea ei fără margini, arătată tătâne-său, și cu mintea ei ageră, a izbutit fata asta să-l scape de la o moarte sigură pe om!\n",
            "- Dacă e așa, atunci mâine, la ora opt, mergem acolo, împreună.\n",
            "Îi duce el, pasărea, craiului - și-l cinstește craiul, și-l laudă peste tot. Vizirul murea de necaz. „Lasă, nu te grăbi - îi zicea în gând feciorului - că nu scapi tu de mine! Ți-o coc eu ție, ai să vezi!“ După care îi mai strecoară măriei-sale:\n",
            "Frumoasa-lumii auzind nenorocirea - că i-au omorât bărbatul, ia urciorul cu apă-vie, își stropește omul și îl învie pe loc. Apoi îi zice:\n",
            "Se așeză pe prispă și se gândi dacă să urce, sau să nu mai urce sus. Își luă inima-n dinți, și intră, că nu era fricos.\n",
            "- Vând, dar pe minciuni!\n",
            "O zăresc, ăia, se năpustesc asupra ei, îi astupă gura cu palma, îi leagă mâinile la spate și - la munte cu ea!\n",
            "- Bani! Vreau bani! zise ăsta din nou.\n",
            "- Mamă, eu nu am tată? Numai ceilalți copii au, de le cumpără tot felul de minunății?\n",
            "- Vino acasă, să-l iei!\n",
            "Împăratul, când auzi așa - se prăbuși \n",
            "cerul pe el. Cu glonțul să-l fi străpuns - și n-ar fi curs sânge din el.\n",
            "Pornisem cam târziu din Băiasa și nu puteam ajunge la Avdela decât noaptea. Eram trei inși, și tustrei cu voie bună. Tovarășii mei de drum îi cunoșteau pe haiduci, pe stăpânii nopților, așa că nu aveam nici o grijă.\n",
            "- Te ascultăm!\n",
            "- Haide, amice, că pe-aici miroase a praf de pușcă! Să spălăm putina!\n",
            "Dar nasul? Cât o pătlăgea vânătă, și - din cauza lui - câte scandaluri, păruieli la fântână, cu muierile care - lua-le-ar naiba, îi ziceau... nas cât cuptorul! Prin tinerețe - s-a certat cu o vecină și patruzeci de ani n-a vorbit cu aia! Să-i zică asta, neobrăzata, că sub nasul ei, al Cuscùsurei adică - s-ar putea adăposti șapte turme de oi?!\n",
            "Cum mai trec anii și cum se mai strică lumea!\n",
            "Se întuneca de-a binelea. Stelele tremurau pe bolta cerului. Vrând, nevrând, Mitu al lui Tegă a rămas la târlă. Ciobanii s-au așternut pe treabă: cineva trase mielul în frigare și aprinse focul, altul a făcut rost de frunziș, de crengi din pădure, pe care le \n",
            "folosi apoi la pregătirea unei colibe și-a unui pat, un pat moale, numai din fèrigă verde, pentru musafir... După care, toți trei s-au lungit pe lângă foc, focul scăzuse de-acum, frigarea se rumenea minunat deasupra jarului bogat, picura din ea grăsime și slobozea abur, ce abur - încărcat de miroazne! Printre arbori, printre crengile lor se furișa luna, și parcă pădurea creștea și se mișca în bătaia razelor ei reci, tăcute, ce umpleau de taină singurătatea. Umbrele munților se lungeau la nesfârșit, sub lună. Câte o pasăre de noapte își scutura aripile peste ei, nevăzută.\n",
            "HAIDA\n",
            "În satele românești S. și M. se porni o nuntă mare, din cele ce încep cu opt zile înainte de cununie și sfârșesc - alte opt zile, după. Celnicul Bucuvală, își însoară feciorul, pe Chita, cu fata celnicului Nastu, din M. În amândouă satele, de la mic până la mare se bucură că celnicii lor fac nuntă. Un singur suflet de om, Haida a lui Nastu, fata după care șapte zile are să fie mireasă, e tare mâhnită și i se rupe inima de jale când se g\n",
            "ândește că într-o zi va fi nevasta lui Chita al lui Bucuvală. Ea, frumoasa și mândra Haidă, ea - singura odraslă a renumitului Nastu - să ia un urât, fără nici un har și bolnăvicios mereu, precum Chita, - mai bine, moartea!\n",
            "Au trecut cincisprezece ani de la logodna Haidei. Avea pe-atunci numai doi ani, iar Chita - cinci. Nastu își juruise fata fiului lui Bucuvală, și de atunci rămaseră cuscri. Și-au crescut copiii, dar pe cât de frumoasă, de minunată se făcea Haida, pe atât de urât, de suferind, slab la trup și la minte ajungea Chita. Dar Bucuvală era avut, turmele-i erau multe, la fel - caravanele de catâri frumoși precum îngerii. Nu era Chitu de scos în lume, dar cu atâta avere - putea trăi bine cu Haida. Așa gândeau Nastu și nevastă-sa - și nici prin cap nu le trecea că Haida n-are să-l vrea pe Chita. Și-apoi, și dacă nu l-ar fi vrut - ce putea face, ar fi îndrăznit să se opună lor? Era cu putință să strice o atât de veche încuscrire? Fata știa treaba asta, și când se gândea ce o așteaptă, ofta din rărunch\n",
            "i și plângea cu lacrimi cât prunele. Iar acum, când vedea că se apropie timpul să intre în casa lui Chita, din zori până în seară tot într-un plâns o ținea.\n",
            "Părinții ei nu înțelegeau de ce le este fata supărată, fiindcă la întrebarea lor - de ce plângi?, ea răspundea că pentru dânșii, că-i va lăsa singuri, și iar o îneca plânsul...\n",
            "Nastu avea un păstor, Chendra, atât de inimos și de chipeș, că-ți venea să-i iei capul și să te tot duci cu el. Și vorbea Chendra ăsta, de parcă avea miere în gură. Haidei îi era de mult drag, și când îl vedea, parcă vedea soarele și, așa, - despre nimica toată - ceasuri întregi ar fi stat cu el de vorbă, de multe ori chiar îl certa, când întârzia să coboare de la oi, iar când venea altcineva după pâine, făcea ea ce făcea și-i dădea prea puțină, ca în felul acesta să-l facă pe Chendra să coboare el în sat.\n",
            "În săptămâna nunții, Chendra era în sat. O vedea el pe Haida cu ochii înlăcrimați, dar nu-i era la îndemână s-o întrebe de ce plânge. Joi, când o împodobiră de mireasă, Haida plâ\n",
            "nse atât de mult încât, seara, când o văzu el, nu se mai putu opri: se apropie de dânsa și, cu vocea lui dulce, îi zise:\n",
            "- Iartă-mă, Haida, că îți vorbesc, dar - rogu-te - spune-mi: ce ai de plângi toată ziua?\n",
            "- Vai, vai - Chendra, tu, Chendra - de-ai fi în inima mea! Dă-te nițel mai aproape, așa - să-ți spun ce necazuri m-apasă.\n",
            "În grădină, sub un cireș, fata, cu ochii înecați în lacrimi, începu să-i povestească:\n",
            "- Tu nu știi, măi băiatule, că ai mei mă mărită, că duminica ce vine, eu am să fiu nevasta urâtului și a blestemului ăla de Chita? Uită-te tu la mine, tu - care mă cunoști de ani de zile: mă potrivesc eu cu Chita? Îmi stă mie bine să ies cu el, să ies în lume cu neisprăvitul acela? Nu-i păcat de mândrețea mea, să-mi închid viața cu unul ca el? Nu-mi trebuie averea lui, nici fruntășia numelui lui, nici casa nu i-o vreau, - dar mi-ar fi drag un om, - să nu mă rușinez cu el când mă scoate în lume. Ah, Chendra frate, - chiar nu băgai de seamă deloc cum îmi înflorea inima când te vedeam că vii de la oi, \n",
            "cum îmi plăcea să stau de vorbă, la nesfârșit, cu tine?\n",
            "Și, plânsul îi curmă vorba. Chendra, bietul, care n-ar fi crezut niciodată că o fată de celnic, mai ales una ca Haida, se poate gândi la el, ba chiar să-l iubească - își înghițise vorbele de tot când auzi ce auzi de la Haida. Tăcea și el, tăcea și Haida, care plângea mereu. Într-un târziu, Haida zise iar:\n",
            "- Ascultă, Chendra - două lucruri mă scapă pe mine de Chita: tu, sau ăsta... Și-n clipa aceea scoase din buzunar un cuțit. Ori fugim amândoi departe, departe - unde nimeni de pe-aici să nu ne vadă, și acolo să trăim împreună după plac, ori duminică, atunci când am să intru în biserica din satul lui Chita, ca să nu mi se pună cununa pe cap - cu cuțitul ăsta am să scap de rușinea pe care vrea să mi-o facă lumea.\n",
            "- Bine, Haida, dar - tu, fată de celnic, o frumusețe și o minte ca a nimănui, cum să te însoțești cu un păstor, ca mine?\n",
            "- Ascultă aici, Chendra: ești băiat bun, harnic și chipeș. Ca pentru inima mea, și de multe ori am fost gata să-i cer tatei să\n",
            " mă dea după tine, dar cunoscându-i năravul... De-aia, Chendra, nu mă lăsa să mor de cuțit. Mîine-i vineri, sâmbătă vin cuscrii să mă ia. Seara ne vom opri la izvoraș, sus, la stâna noastră. Acolo, toți ceilalți au să petreacă, au să cânte, să joace... Eu pândesc clipa potrivită, mă furișez de lângă cuscrime, tu vii după mine și - amândoi, fugim, să scăpăm de lumea asta rea. Ce zici?\n",
            "Chendra ce să zică, a rămas ca prostit, auzind aceste vorbe, și, după ce se gândi nițel, îi zise fetei:\n",
            "- Haida mea, ori mor și eu cu tine, ori fugim împreună sâmbătă noaptea. Fă-te mireasă tu, Haido, și - Dumnezeu cu noi!\n",
            "- Să ne-ajute Dumnezeu! - rosti și fata, și se despărțiră. Haida se mai liniștește, acum nu mai plânge, ba - cu inima inundată de bucurie, se gătește mireasă, ca toate miresele...\n",
            "Sosește și ziua de sâmbătă, iar cuscrii, mulți, mulți ca frunza, veniră să-și ia mireasa. Haida, în rochie de mătase și batic la fel, cu voalul pe cap, cu salba de galbeni până la brâu, ai fi zis că e o zână. Au urcat-o pe cal și au p\n",
            "ornit către satul lui Chita. Era atâta lume, cum nu se mai văzuse la altă nuntă. Chendra purta calul miresei de căpăstru, dar dacă te uitai bine la el, se vedea cât de colo că îl încercau grele gânduri.\n",
            "Înspre seară au ajuns la stâna lui Nastu și, acolo, zece păstori tăiau și curățau noateni, în timp ce tot atâția potriveau jarul, să-i pună în frigare. Cinci cai încărcați cu butoiașe de vin fură și ei despovărați, - dar Nastu tot zicea că n-ar fi de-ajuns. Petrecerea a început cu țuică, iar când au fost gata frigările - cincisprezece păstori turnau vin la lumea care închina - și tot nu erau destui. Cântau și țopăiau cuscrii, de ți se părea că te afli pe cine știe ce lume!\n",
            "Timpul se scurgea și, când vinul era pe sfârșite, puțină lume mai rămăsese trează. Haida se prefăcu a dormi, și când toți cei din jurul dânsei închiseră și ei ochii, - așa cum era îmbrăcată, mireasă - se furișă ușurel de lângă cuscri și se repezi apoi în jos, spre vale, unde coborâse și Chendra. Amândoi, - nopți și zile întregi s-au tot dus,\n",
            " departe, cale de săptămâni întregi.\n",
            "Nunta s-a spart. Nastu, cu ai lui, se întoarse în sat. Dar, de-atâta rușine, bietul om - a îmbătrânit numai într-o zi, de parcă trecuseră ani nenumărați peste el. Ajuns în sat, porunci ca în casa lui, niciodată să nu mai fie amintit numele Haidei.\n",
            "Au trecut de atunci douăzecișicinci de ani. Era într-o zi de Sâmpetru, către amiază. Aromânii din satul M. ieșiseră aproape toți în livada de lângă așezarea lor, unde tinerii se prinseseră în horă, iar bătrânii priveau la ei, de pe margine, multora dintre dânșii părându-le tare rău că nu-i ajută picioarele, să mai țopăie și ei puțintel.\n",
            "Din zare se vedeau trei călători, care veneau călare. S-apropiară și se opriră chiar în dreptul bătrânilor satului. În fruntea lor era un tânăr, mândru și frumos ca un înger. Avea atâta demnitate în privire, încât bătrânii se ridicară în picioare când băiatul, coborând de pe cal, le dădu ziuă bună. Hora se opri și ea, și lumea toată avu ochi numai pentru călători. După îmbrăcăminte, aceștia nu păr\n",
            "eau a fi oameni de pe-aici, aproape. După binețe, cum e obiceiul aromânesc, unul dintre bătrâni îi zise tânărului călător că în sat nu este han, dar dacă vrea să-i cinstească casa, îl poftește să-i fie oaspete în seara asta. Tânărul porunci tovarășilor săi să-i ia caii și, însoțiți de un flăcău, îi duseră în sat. Când s-a spart hora, către seară, călătorul, cu o ceată de oameni, s-a dus la gazda care-l invitase pentru popas, unde-au rămas o vreme.\n",
            "La înfățișare, acest tânăr aducea cu cineva din sat - și mulți bătrâni au vorbit între ei despre asta, dar nimeni n-a putut desluși ca lumea cu cine anume ar semăna.\n",
            "A doua zi de dimineață, la biserică, ca să-l cinstească pe tânăr, îl așezară lângă fruntașii satului. Iar lângă el chiar, ședea bâtul Nastu, care cât a ținut slujba nu și-a luat ochii de la călătorul cel tânăr.\n",
            "Din ziua aceea neagră, când a pățit Nastu rușinea fără seamăn, de i-a fugit fie-sa, de multe ori - oameni din sat, ca și străini, vrură să aducă vorba despre Haida, dar el curma orice discuție - \n",
            "cine vrea să-mi fie la inimă, zicea el - să nu-mi vorbească despre asta. Nici Nastu însuși nu aducea vreodată vorba, în casă, despre fată.\n",
            "Acum, bătrânul, cu cât îl privea mai mult pe băiat, cu atât simțea că vrea, din inimă, să stea de vorbă cu el.\n",
            "La ieșirea din biserică, oamenii, în curte, în grupuri de câte cinci-șase, își rânduiau vizitele de făcut, iar mai la o parte, moșul Nastu, sprijinit în toiag, trăgea cu coada ochiului tot la străinul cel tânăr. Când se ivi din biserică și bătrâna, bâtul o chemă lângă el și, după ce-i șopti câteva vorbe, își îndreptă și dânsa privirile spre călător. Parcă i se lua și bătrânei o piatră de pe suflet.\n",
            "Lumea se risipi, și cei doi bătrâni plecară și ei, încet-încet.\n",
            "Ajunși acasă, bătrâna - ca niciodată - nu mai știa cum să-și împodobească mai bine odaia de oaspeți. Îi era inima numai bucurie și chiar îi spunea că în casa asta are să se petreacă, astăzi, ceva nou, ceva bun. Din când în când, ieșea până la poartă, să vadă de nu cumva, străinul acela se îndreaptă spre ei.\n",
            "\n",
            "Către prânz, într-adevăr, o ceată de oameni cu tânărul călător între ei, veniră în vizită și la moșul Nastu. Bătrânei îi tremurau picioarele, iar bâtului îi bătea inima atât de tare, de parcă ar fi stat să-i iasă din piept. S-a vorbit de una, de alta, dar Nastu nu era deloc atent, el se tot frământa cum să facă să-l întrebe pe străin de pe unde-i. Tânărul îl ajută, fiindcă începu chiar el, despre asta.\n",
            "- Bâtule, am luat seama, încă din biserică, la dumneata, că te uiți la mine dinadins. Poate vrei să știi cine sunt. Eu vin de departe tare. Acolo la noi trăiește un celnic mare. I-a dat Dumnezeu de toate, dar nevasta lui are un of nestins: e prea departe de părinți și prea nu i-a văzut de mult. Știind dânsa că trec și pe-aici, mi-a cerut să vă aduc urările lor de bine - matale și nevestei matale, multe urări de bine. Dar de ce domniilor voastre cu precădere și nu altcuiva de-aici din sat, nu știu.\n",
            "- Spune, dragul mamei, îi zise maia Nastu, nu cumva o cheamă Haida pe femeia ceea? Și-o fi și rudă în vreun fel cu\n",
            " tine?\n",
            "- De-aș ști că vă sunt dragi celnicul acela și nevasta lui, Haida, v-aș spune și ce rudenie am eu cu ei.\n",
            "- Cum să nu ne fie dragă Haida, fiule, se repezi bătrâna - de vreme ce ea este fata noastră?\n",
            "- Atunci aflați că eu sunt băiatul Haidei, și vin să aflu din gura voastră - dacă ați iertat-o...\n",
            "Bătrâna nu se mai putu stăpâni; se năpusti la nepotu-său și începu să-l sărute, de nu se mai sătura. Bietul moș Nastu, ai fi zis că încremenise acolo unde se afla așezat. Unul dintre oaspeți îl luă de braț și îl duse la nepotul lui, care îi sărută mâna, iar moșul, la rându-i, îl sărută pe băiat pe frunte, în timp ce ochii i se umplură de lacrimi.\n",
            "Și la prânz, și la cină, nepotul a fost oaspetele moșului Nastu, iar după alte două luni de zile, celnicul Chendra cu Haida, cu toți ai casei, cu turmele de oi, caravanele de cai - au venit în satul M.\n",
            "Casa bătrânului Nastu, zăvorâtă de atâția ani de zile, s-a deschis din nou pentru lume. Chendra și Haida, de treabă cum erau ei și-n tinerețe, îi iubeau și îi cinsteau pe\n",
            " bătrâni nemaivăzut. Aceștia, care nu mai voiseră să știe de nimic, în singurătatea lor îndelungă, acum nu se mai săturau de viață - și au mai trăit ani mulți încă, de au crescut nepoți și nepoți de nepoți.\n",
            "NEPOATA UNUI PAZNIC\n",
            "Munții rămăseseră pustii.\n",
            "Au coborât turmele de oi la iernat... iată, ciobanii, mâhniți, nu se mai zăresc... familii - în caravane, au plecat și ele... Satul, singur de-acum, își căinează soarta. Pădurile vuiesc, parcă ar sta să-și mărturisească dorul lor pentru prietenii plecați departe, departe, care au adus cu dânșii bucuriile, nădejdile locului.\n",
            "Au amuțit dragile cântece din fluier, nu se mai aud mândrele aromânce fredonând. Le-a luat locul vântul tăios, care mătură frunzele veștede și umple satul întreg cu ele... Vântul trezește satul ca dintr-un vis greu, și trece apoi mai departe, șuierând prin văioagele pietroase ale munților.\n",
            "Câtă tristețe coboară toamna aici, câți dintre cei ce se-ndrăgesc nu desparte ea... câte iubiri nu face să lâncezească și să se stingă chiar, pentru totde\n",
            "auna!\n",
            "Ploaia măruntă și rece, ce părea că nu va conteni vreodată, a stat acum. Sus, printre spărturile norilor zdrențăroși, se strecoară firave raze de soare, pierite aproape, în timp ce turme și oameni și caravane suie trudnic panta muntelui.\n",
            "Gospodarii își îndemnau caii, știind că dincolo de curmătură se întinde șesul unde vor face popasul odihnitor.\n",
            "Prin văi adânci - soarele nu pătrunde deloc. Umbrele încep a se lungi peste munți și peste prăpăstii, mai întâi ca o lumină cenușie, apoi ca un giulgiu negru... negru, sub care pădurile dispar în tăcere. Câmpia, și ea cufundată în întuneric, visează frumoase zile de primăvară... E o liniște de țintirim. Dar, deodată pădurea răsună de voci omenești, de sunet de talăngi, lătrat de câini: cei plecați la iernat - poposesc undeva.\n",
            "Focuri luminează acum câmpia. Catârii fură ușurați de poveri, oile se revărsară după iarbă înrourată. Cerul, înorat mai devreme, se însenină pe nebăgate de seamă; și cu el se înseninară și sufletele ciobanilor, mohorâți până aici. Fluierel\n",
            "e începură a doini despre dorurile celor tineri; cântecele flăcăilor, făceau să răsune câmpia până departe. Munții și văile se trezeau; ca și pădurile. Totul în jur se înveselea - de voia bună nevinovată a oamenilor. Luna s-a urcat în înaltul cerului, iar focurile piereau rând pe rând. Armânii dormeau acum, somn ușor, sub cerul deschis - ca viața lor.\n",
            "Numai un suflet pătimea în acest timp, un tânăr mlădiu și plin de vigoare, ca toți oamenii muntelui. Păzea focul arzând... arzând ca inima sa. Nu scotea o vorbă, chinuit de gânduri ce-i veneau amestecate, tulburi, fără nici o rânduială, că nu se-nțelegea nimic din ele; se-nșirau așa, să-i deslușești în ele viața-i chinuită: uite-l mic de tot, orfan, apoi mai mare, singur - neavând pe nimeni să i se destăinuie... Și, într-o zi de Sântă-Măria, îi ieși în cale o inimă dragă, o fată cu suflet de înger, da - așa era - cu care s-a înțeles dintr-o dată: și-au împărtășit unul altuia dorurile, au plâns de bucurie amândoi, și de-atunci... La gândul ăsta, fruntea i se îmbr\n",
            "obonă de sudoare rece, ochii i se umeziră... și, fără să mai stea pe gânduri, se ridică în picioare și se uită, lung, înspre sat.\n",
            "În jur, liniște grea: pe boltă, stelele licăreau ușor, luna strălucind își vedea de drumul ei, umplând pădurile cu lumină. Iar aromânii dormeau somn dulce... Liniște adâncă și-n cer, și pe pământ; dar în pieptul lui Dinu mocnea furtuna. Liniștea locului însă, parcă-i mai alina uraganul din suflet, și așa - se așeză din nou lângă foc, să macine mai departe gânduri. Dar și acum, tot gânduri negre, ba mai negre chiar, îl copleșeau... îi răsunau în urechi plânsete... și umbre lungi, spăimoase îi jucau pe dinainte... Închise o clipă ochii, să nu mai vadă... și-atunci i se-nchegă în minte icoana curată a celei care-i frământa atâta sufletul: departe, în sat, draga lui, Vira, rămăsese singură, cu tatăl ei - paznicul satului, să-și plângă zilele, și viața ei, numai necazuri, și ce-o să-i mai aducă iarna...\n",
            "Imaginea iubitei îl dădu gata... și n-a mai stat pe gânduri. Se ridică, parcă mușcat\n",
            " de vreun vis rău, puse mâna pe-o bâtă, își chemă câinele ciobănesc de lângă oi, și porni spre sat, să-și bucure fata care-i înțelese dorul și lacrimile, s-o scape de trai greu... Se grăbea, ca un străin ajuns aproape de locurile lui, care e nerăbdător să și-i îmbrățișeze cât mai repede pe ai săi.\n",
            "Satul, pierdut în întuneric, visa un vis cenușiu, de toamnă. Dinspre pădure sufla tăios un crivăț turbat, care sta să smulgă copacii din rădăcină, zguduind totul în calea lui; parcă niște iele, scăpate din lanțuri grele - se năpustesc peste așezările oamenilor. Începe să fulguie, și neaua se așternu în straturi peste satul tot. Gospodăriile erau goale, întunecate, doar o căsuță, cu o curte mare și pustie, era luminată de-o candelă... Înăuntru, lângă vatră, lucra o fată cu obrajii aprinși ca jarul, cu ochii mari și îndurerați, cu părul revărsat, de mătase; era ca un măr de aur aruncat aici, în sărăcia casei. Fata lucra, draga de ea, iar lacrimile ce-i șiroiau pe față ar fi putut stinge jarul din focul de alături. De-\n",
            "afară, bufnituri de zăpadă izbeau cu putere ferestrele cârpite cu hârtii... Fata, apăsată de gânduri, își simțea pe obraji lacrimile calde.\n",
            "Lungă și tristă e povestea acestui suflet curat, copleșit de necazuri: rămasă încă de mică fără părinți, a fost luată de suflet, de către paznicul satului, un bețivan care o lăsa zile în șir singură, singură în casă, doar cu un codru de pâine uscată; iar când bețivul ăsta se întorcea de pe unde umbla, trebuia să-i îndure brațul lui greu, fiindcă o bătea... O bătea, până o lăsa lată, pe jos. Și fata i le răbda toate astea... I le răbda, nădăjduind că într-o bună zi avea să se mântuie ea de traiul ăsta câinesc; pentru că se mai afla undeva o ființă care se gândea la dânsa... Era dragul ei, Dinu, care de-atâtea ori i-a vorbit de-o altfel de viață, mai fericită...\n",
            "Iată la ce se gândea biata fată, în noaptea aceea neagră, și-i curgeau lacrimile, fără contenire...\n",
            "Trecuse de miezul-nopții, iar afară ninsoarea se oprise de tot; vântul murise și el, iar satul zăcea sub giulgiul a\n",
            "lb de zăpadă.\n",
            "Fata lucra încă, lângă focul mocnit... și de nedormită ce era - nu mai putea închide ochii, mai ales - cutreierată de gânduri: uneori parcă-l vedea pe iubitul ei Dinu departe, pe câmp, și-i simțea cumva privirile îndreptate către dânsa... în alte dăți părea că aude glasul lui dulce, chemând-o să fugă cu el... să fugă - cine știe unde și să scape de viața asta grea... Dar, n-a auzit acum, aievea, fluierul lui Dinu? Doinea de jale, și cântecul cobora dinspre dealuri, răsunând frumos, până aici, în odaia ei. O cuprinse deodată un tremur, și un râs ciudat îi înflori buzele, apoi se prăbuși sfârșită pe vatra rece.\n",
            "Afară, satul încă se mai înfiora de cântecul doinit din fluier, de Dinu, fiindcă el era cântărețul... Plecat din câmp, merse ceasuri întregi, și - către zori, intră pe uliți. A vrut să se ducă de-a dreptul la casa Virei, s-o bucure - că o știa necăjită, dar tatăl ei... sufletul ăla hain l-ar fi simțit, și-atunci era și mai rău de dânsa.\n",
            "De aceea, când fu pe muchia dealului, flăcăul mai slob\n",
            "ozi o dată cântecul lor drag, prin care îi dădea de veste și vara, de la târlă.\n",
            "Așteptă o clipă... Dar, dinspre casa Virei - nici un semn, nici un glas, doar o luminiță pâlpâind înăuntru. A vrut să mai zică din fluier o dată, dar fluierul parcă murise, n-a mai scos un sunet... Îl azvârli pe zăpadă, și începu din gură, chiar cântecul Virei, pe care îl zicea ea când se ducea la târlă ori la fântână:\n",
            "Fată - ochi de peruzea,\n",
            "Draga mea armână,\n",
            "Ia nu mai întârzia\n",
            "Seara, la fântână.\n",
            "Și, ia nu te mai găti\n",
            "Toată, în mătasă,\n",
            "Că feciorii te-or răpi,\n",
            "La-nturnat acasă.\n",
            "Și să nu mai porți, la hori,\n",
            "Gălbenei anume,\n",
            "Că-i smintești pe cei feciori -\n",
            "Și-or roi în lume...\n",
            "Și cânta Dinu cu atâta foc, cu atâta dor, că până și satul pustiu se îndurera. Casele goale și tăcute se treziră, la vocea lui, cunoscută lor, și-l îngânară... parcă și ele sufereau cu dânsul. Se opri puțin, așteptând. Simțea o bucurie de copil mare, când se gândea că peste puțin are să fie lângă draga lui și o să-i șteargă lacrimile de pe obraji... Dar vremea \n",
            "parcă zbura... iată, dinspre răsărit, zorile stau să se reverse. Simțind asta, Dinu se grăbi să coboare.\n",
            "Era cam frig, și din când în când vântul sufla cu putere izbindu-l pe băiat în față; Dinu însă pășea mai departe prin satul cufundat încă în întunericul dinaintea zorilor. Ajuns lângă casa celuilalt paznic, se uită pe fereastră: în vatră ardea un foc zdravăn, iar paznicii se tolăniseră alături.\n",
            "Un câine l-a simțit, a lătrat, s-a ținut după el și s-a întors repede. Dinu a mers mai departe, fără să se uite în urmă-i... dorea să fie cât mai degrabă acolo unde-și pusese în gând sa ajungă.\n",
            "Lătratul câinelui îi trezi pe paznicii amețiți de vinul băut o noapte întreagă. Și-au dat seama că a intrat cineva în sat, vreun străin. Unchiul Virei puse mâna pe-o secure cât toate zilele, luă și o bucată de zadă - s-o aprindă la nevoie - și țâșni afară. Aci, observând zăpada călcată în picioare - nu-i mai chemă pe ceilalți și alergă spre gospodăria sa. Frigul de dimineață îl trezi și mai mult, ba chiar îl și învigoră.\n",
            "Apro\n",
            "ape de casă, bagă de seamă că o umbră se tot mișca... Se apropiè de dânsa... Capul îi era greu ca de plumb, picioarele nu prea îl țineau, dar își simți brațele încordate cum se ridică și lovesc cu putere, fără vrerea lui... Securea trăsni și, pe loc - un trup înalt se rostogoli greoi în zăpadă, în timp ce un glas stins a murmurat abia auzit: „Ah, Doamne - s-a zis cu mine!“\n",
            "Paznicul se înfioră de groază... Vocea îi era cunoscută; și, și-a înțeles, tocmai de aceea, fapta nesăbuită... Era pierit, nu mai știa ce să facă, și - deodată, cu securea în mână, cu sarica și cămașa stropite de sânge - alergă în casă!\n",
            "Vira se trezi, înspăimântată. Văzându-l pe unchi-său plin de sânge, a țipat... El a înălțat securea și-a zis încet, amenințător:\n",
            "- Taci... taci - că l-am mâncat fript...\n",
            "Vira a tăcut... Și, deși nu înțelegea nimic din ce se petrecea, nici nu îndrăzni să întrebe ceva.\n",
            "Paznicul, amețit încă de vin, și - poate în mai mare măsură - de sângele de pe cămașă și de pe sarică... nu mai rezistă, căzu într-un colț și n\n",
            "umaidecât, ca trăsnit, adormi... după nopți de nesomn și de beție.\n",
            "Spre răsărit, se lumina de ziuă. Casa era la loc deschis și nu se aflase nici până atunci chiar în beznă. Vira, parcă era de pe altă lume; nu-și amintea nimic, nu înțelegea nimic din ce vedea în jur.\n",
            "Afară vântul șuiera prelung, parcă-ar fi cobit... Vira dădu să iasă în curte, dar se întoarse numaidecât ca mușcată de șarpe, când zări zăpada înroșită, ba mai auzi și-o voce îngrozitoare, care ajungea la ea... Rămăsese înmărmurită, în casă... Îi venea a plânge și nu știa de ce; s-ar fi gândit la ceva, dar la fel - nu știa la ce, pentru că totul era fără noimă, încurcat, pentru dânsa.\n",
            "Alături, unchiu-său sforăia în somn monstruos, ceea ce o sperie și mai rău. A ieșit iar afară și a pornit pe urmele sângelui. Abia așa înțelesese... Un lac de sânge și - în mijlocul lui - dragul ei Dinu, cu fața în sus, întins pe jos; Vira țipă scurt... și alunecă lângă băiat.\n",
            "Vântul le mângâia lin obrajii... Parcă-i căinà... Îi căinà pe acești doi tineri chinuiți, c\n",
            "u sufletele lor curate, de îngeri adevărați.\n",
            "Lumina se revarsă peste sat. Dinu și Vira stau tot în zăpadă, iar paznicii dorm duși, somn greu; pe ulițe, nici țipenie. Dar vântul se înteți suflând și mai rece; fulgi de nea cădeau rar peste cei doi. Fulgii de nea o mai treziră pe fată, care începu să dea semne de viață... Își revenea din leșin. Câteva clipe mai târziu se mișcă neliniștită, deschise ochii mari și țâșni ca arsă! Se simțea ușoară și în putere iar... cânta, țipa... râdea în hohote, ca niciodată în viața ei. Apoi, înșfăcându-l pe Dinu, îl trase în casă. Îl lăsă acolo, și iar hohoti în râs..., iar țipete, cântec, după care ieși din nou din casă. A început să alerge, desculță, prin sat... Alerga spre biserică. Aci trase cu putere clopotul cel mare, ca și când își pusese în gând să adune tot satul - să-l plângă pe dragul ei Dinu...\n",
            "La dangătul clopotului, satul răsuna - pustiu... Apoi tăcerea se înstăpâni din nou...\n",
            "Vira se întoarse acasă... Cânta, râdea... Își îmbrăcă straiele de sărbătoare și ieși din\n",
            " nou pe uliță...\n",
            "Fata, biata de ea, înnebunise.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file_path = 'C:/Users/gheto/Desktop/PoS/AromanianPoS/dataset/Tales.test.ro'\n",
        "for text_chunk in aromanian_iterator(file_path):\n",
        "    # Process each text chunk\n",
        "    print(text_chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "shxXAotV4v5m"
      },
      "outputs": [],
      "source": [
        "def get_data(train_dir='C:/Users/gheto/Desktop/PoS/AromanianPoS/dataset/Tales.train.ro', valid_dir='C:/Users/gheto/Desktop/PoS/AromanianPoS/dataset/Tales.test.ro'):\n",
        "    # gets the data\n",
        "    train_iter = aromanian_iterator(train_dir)\n",
        "    train_iter = to_map_style_dataset(train_iter)\n",
        "    valid_iter = aromanian_iterator(valid_dir)\n",
        "    valid_iter = to_map_style_dataset(valid_iter)\n",
        "\n",
        "    return train_iter, valid_iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "id": "wKZNNoSto6GK"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Word2VecParams:\n",
        "\n",
        "    # skipgram parameters\n",
        "    MIN_FREQ = 1  # aici trebuie sa fie 1, altfel o sa fie token-uri in context care nu sunt si in center\n",
        "    SKIPGRAM_N_WORDS = 20\n",
        "    T = 85\n",
        "    NEG_SAMPLES = 1\n",
        "    NS_ARRAY_LEN = 5_000_000\n",
        "    SPECIALS = \"<unk>\"\n",
        "    TOKENIZER = 'basic_english'\n",
        "\n",
        "    # network parameters\n",
        "    BATCH_SIZE = 100\n",
        "    EMBED_DIM = 300\n",
        "    EMBED_MAX_NORM = None\n",
        "    N_EPOCHS = 100\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    CRITERION = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "id": "FB5Kh3LCqBDE"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self, list, specials):\n",
        "        self.stoi = {v[0]:(k, v[1]) for k, v in enumerate(list)}\n",
        "        self.itos = {k:(v[0], v[1]) for k, v in enumerate(list)}\n",
        "        self._specials = specials[0]\n",
        "        self.total_tokens = np.nansum(\n",
        "            [f for _, (_, f) in self.stoi.items()]\n",
        "            , dtype=int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.stoi) - 1\n",
        "\n",
        "    def get_index(self, word: Union[str, List]):\n",
        "        if isinstance(word, str):\n",
        "            if word in self.stoi:\n",
        "                return self.stoi.get(word)[0]\n",
        "            else:\n",
        "                return self.stoi.get(self._specials)[0]\n",
        "        elif isinstance(word, list):\n",
        "            res = []\n",
        "            for w in word:\n",
        "                if w in self.stoi:\n",
        "                    res.append(self.stoi.get(w)[0])\n",
        "                else:\n",
        "                    res.append(self.stoi.get(self._specials)[0])\n",
        "            return res\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Word {word} is not a string or a list of strings.\"\n",
        "                )\n",
        "\n",
        "\n",
        "    def get_freq(self, word: Union[str, List]):\n",
        "        if isinstance(word, str):\n",
        "            if word in self.stoi:\n",
        "                return self.stoi.get(word)[1]\n",
        "            else:\n",
        "                return self.stoi.get(self._specials)[1]\n",
        "        elif isinstance(word, list):\n",
        "            res = []\n",
        "            for w in word:\n",
        "                if w in self.stoi:\n",
        "                    res.append(self.stoi.get(w)[1])\n",
        "                else:\n",
        "                    res.append(self.stoi.get(self._specials)[1])\n",
        "            return res\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Word {word} is not a string or a list of strings.\"\n",
        "                )\n",
        "\n",
        "\n",
        "    def lookup_token(self, token: Union[int, List]):\n",
        "        if isinstance(token, (int, np.int64)):\n",
        "            if token in self.itos:\n",
        "                return self.itos.get(token)[0]\n",
        "            else:\n",
        "                raise ValueError(f\"Token {token} not in vocabulary\")\n",
        "        elif isinstance(token, list):\n",
        "            res = []\n",
        "            for t in token:\n",
        "                if t in self.itos:\n",
        "                    res.append(self.itos.get(token)[0])\n",
        "                else:\n",
        "                    raise ValueError(f\"Token {t} is not a valid index.\")\n",
        "            return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "id": "iJp-0yudpFfR"
      },
      "outputs": [],
      "source": [
        "def yield_tokens(iterator, tokenizer):\n",
        "    r = re.compile('[a-z1-9]')\n",
        "    for text in iterator:\n",
        "        res = tokenizer(text)\n",
        "        res = list(filter(r.match, res))\n",
        "        yield res\n",
        "\n",
        "def vocab(ordered_dict: Dict, min_freq: int = 1, specials: str = '<unk>'):\n",
        "    tokens = []\n",
        "    # Save room for special tokens\n",
        "    for token, freq in ordered_dict.items():\n",
        "        if freq >= min_freq:\n",
        "            tokens.append((token, freq))\n",
        "\n",
        "    specials = (specials, np.nan)\n",
        "    tokens[0] = specials\n",
        "\n",
        "    return Vocab(tokens, specials)\n",
        "\n",
        "def pipeline(word, vocab, tokenizer):\n",
        "    return vocab(tokenizer(word))\n",
        "\n",
        "def build_vocab(\n",
        "        iterator,\n",
        "        tokenizer,\n",
        "        params: Word2VecParams,\n",
        "        max_tokens: Optional[int] = None,\n",
        "    ):\n",
        "    counter = Counter()\n",
        "    for tokens in yield_tokens(iterator, tokenizer):\n",
        "        counter.update(tokens)\n",
        "\n",
        "    # First sort by descending frequency, then lexicographically\n",
        "    sorted_by_freq_tuples = sorted(\n",
        "        counter.items(), key=lambda x: (-x[1], x[0])\n",
        "        )\n",
        "\n",
        "\n",
        "    # Aici el le ordoneaza dupa frecventa, dar daca facem asta se cam strica alinierea\n",
        "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "\n",
        "    # ordered_dict = OrderedDict(counter.items())\n",
        "\n",
        "    word_vocab = vocab(\n",
        "        ordered_dict, min_freq=params.MIN_FREQ, specials=params.SPECIALS\n",
        "        )\n",
        "    \n",
        "    return word_vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "V-5vtReKtlg0"
      },
      "outputs": [],
      "source": [
        "class SkipGrams:\n",
        "    def __init__(self, vocab: Vocab, vocab_context: Vocab, params: Word2VecParams, tokenizer):\n",
        "        self.vocab = vocab\n",
        "        self.vocab_context = vocab_context\n",
        "        self.params = params\n",
        "        self.t = self._t()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.discard_probs = self._create_discard_dict()\n",
        "\n",
        "    def _t(self):\n",
        "        freq_list = []\n",
        "        for _, (_, freq) in list(self.vocab.stoi.items())[1:]:\n",
        "            freq_list.append(freq/self.vocab.total_tokens)\n",
        "        return np.percentile(freq_list, self.params.T)\n",
        "\n",
        "\n",
        "    def _create_discard_dict(self):\n",
        "        discard_dict = {}\n",
        "        for _, (word, freq) in self.vocab.stoi.items():\n",
        "            dicard_prob = 1-np.sqrt(\n",
        "                self.t / (freq/self.vocab.total_tokens + self.t))\n",
        "            discard_dict[word] = dicard_prob\n",
        "        return discard_dict\n",
        "\n",
        "\n",
        "    def collate_skipgram(self, batch):\n",
        "        batch_input, batch_output  = [], []\n",
        "        for text in batch:\n",
        "            # Chestia asta e o lista cu index-ul fiecarui cuvant\n",
        "            text_tokens = self.vocab.get_index(self.tokenizer(text))\n",
        "            text_tokens_context = []\n",
        "            for text in text_tokens:\n",
        "                context_word = self.vocab_context.lookup_token(text)\n",
        "                text_tokens_context.append(self.vocab_context.get_index(context_word))\n",
        "    \n",
        "            \n",
        "\n",
        "            if len(text_tokens_context) < self.params.SKIPGRAM_N_WORDS * 2 + 1:\n",
        "                continue\n",
        "\n",
        "            for idx in range(len(text_tokens_context) - self.params.SKIPGRAM_N_WORDS*2\n",
        "                ):\n",
        "                token_id_sequence = text_tokens[\n",
        "                    idx : (idx + self.params.SKIPGRAM_N_WORDS * 2 + 1)\n",
        "                    ]\n",
        "                \n",
        "                # Aici e scos cuvantul central, dar eu l-as pastra si ca sa nu strict codul ii dau append dupa\n",
        "                input_ = token_id_sequence.pop(self.params.SKIPGRAM_N_WORDS)\n",
        "                outputs = token_id_sequence\n",
        "\n",
        "                # L-am adaugat inapoi aici\n",
        "                outputs.append(input_)\n",
        "\n",
        "\n",
        "                prb = random.random()\n",
        "                del_pair = self.discard_probs.get(input_)\n",
        "                if input_==0 or del_pair >= prb:\n",
        "                    continue\n",
        "                else:\n",
        "                    for output in outputs:\n",
        "                        prb = random.random()\n",
        "                        del_pair = self.discard_probs.get(output)\n",
        "                        if output==0 or del_pair >= prb:\n",
        "                            continue\n",
        "                        else:\n",
        "                            batch_input.append(input_)\n",
        "                            batch_output.append(output)\n",
        "\n",
        "        batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
        "        batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
        "\n",
        "        return batch_input, batch_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "id": "0mTWsnXTxTXz"
      },
      "outputs": [],
      "source": [
        "class NegativeSampler:\n",
        "    def __init__(self, vocab: Vocab, ns_exponent: float, ns_array_len: int):\n",
        "        self.vocab = vocab\n",
        "        self.ns_exponent = ns_exponent\n",
        "        self.ns_array_len = ns_array_len\n",
        "        self.ns_array = self._create_negative_sampling()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ns_array)\n",
        "\n",
        "    def _create_negative_sampling(self):\n",
        "\n",
        "        frequency_dict = {word:freq**(self.ns_exponent) \\\n",
        "                          for _,(word, freq) in\n",
        "                          list(self.vocab.stoi.items())[1:]}\n",
        "        frequency_dict_scaled = {\n",
        "            word:\n",
        "            max(1,int((freq/self.vocab.total_tokens)*self.ns_array_len))\n",
        "            for word, freq in frequency_dict.items()\n",
        "            }\n",
        "        ns_array = []\n",
        "        for word, freq in tqdm(frequency_dict_scaled.items()):\n",
        "            ns_array = ns_array + [word]*freq\n",
        "        return ns_array\n",
        "\n",
        "    def sample(self,n_batches: int=1, n_samples: int=1):\n",
        "        samples = []\n",
        "        for _ in range(n_batches):\n",
        "            samples.append(random.sample(self.ns_array, n_samples))\n",
        "        samples = torch.as_tensor(np.array(samples))\n",
        "        return samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "id": "a-Xjg40wvtag"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab: Vocab, vocab_context:Vocab, params: Word2VecParams):\n",
        "        super().__init__()\n",
        "        self.vocab = vocab\n",
        "        self.vocab_context = vocab_context\n",
        "        self.t_embeddings = nn.Embedding(\n",
        "            self.vocab.__len__()+1,\n",
        "            params.EMBED_DIM,\n",
        "            max_norm=params.EMBED_MAX_NORM\n",
        "            )\n",
        "        self.c_embeddings = nn.Embedding(\n",
        "            self.vocab_context.__len__()+1,\n",
        "            params.EMBED_DIM,\n",
        "            max_norm=params.EMBED_MAX_NORM\n",
        "            )\n",
        "\n",
        "    def forward(self, inputs, context):\n",
        "        # getting embeddings for target & reshaping\n",
        "        target_embeddings = self.t_embeddings(inputs)\n",
        "        n_examples = target_embeddings.shape[0]\n",
        "        n_dimensions = target_embeddings.shape[1]\n",
        "        target_embeddings = target_embeddings.view(n_examples, 1, n_dimensions)\n",
        "\n",
        "        # get embeddings for context labels & reshaping\n",
        "        # Allows us to do a bunch of matrix multiplications\n",
        "        context_embeddings = self.c_embeddings(context)\n",
        "        # * This transposes each batch\n",
        "        context_embeddings = context_embeddings.permute(0,2,1)\n",
        "\n",
        "        # * custom linear layer\n",
        "        dots = target_embeddings.bmm(context_embeddings)\n",
        "        dots = dots.view(dots.shape[0], dots.shape[2])\n",
        "        return dots\n",
        "\n",
        "    def normalize_embeddings(self):\n",
        "        embeddings = list(self.t_embeddings.parameters())[0]\n",
        "        embeddings = embeddings.cpu().detach().numpy()\n",
        "        norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
        "        norms = norms.reshape(norms.shape[0], 1)\n",
        "        return embeddings / norms\n",
        "\n",
        "    def get_similar_words(self, word, n):\n",
        "        word_id = self.vocab.get_index(word)\n",
        "        if word_id == 0:\n",
        "            print(\"Out of vocabulary word\")\n",
        "            return\n",
        "\n",
        "        embedding_norms = self.normalize_embeddings()\n",
        "        word_vec = embedding_norms[word_id]\n",
        "        word_vec = np.reshape(word_vec, (word_vec.shape[0], 1))\n",
        "        dists = np.matmul(embedding_norms, word_vec).flatten()\n",
        "        topN_ids = np.argsort(-dists)[1 : n + 1]\n",
        "\n",
        "        topN_dict = {}\n",
        "        for sim_word_id in topN_ids:\n",
        "            sim_word = self.vocab_context.lookup_token(sim_word_id)\n",
        "            topN_dict[sim_word] = dists[sim_word_id]\n",
        "        return topN_dict\n",
        "\n",
        "    def get_similarity(self, word1, word2):\n",
        "        idx1 = self.vocab.get_index(word1)\n",
        "        idx2 = self.vocab_context.get_index(word2)\n",
        "        if idx1 == 0 or idx2 == 0:\n",
        "            print(\"One or both words are out of vocabulary\")\n",
        "            return\n",
        "\n",
        "        embedding_norms = self.normalize_embeddings()\n",
        "        word1_vec, word2_vec = embedding_norms[idx1], embedding_norms[idx2]\n",
        "\n",
        "        return cosine(word1_vec, word2_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {
        "id": "mMx67oLYwS9x"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model: Model, params: Word2VecParams, optimizer,\n",
        "                vocab: Vocab, train_iter, valid_iter, skipgrams: SkipGrams):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.vocab = vocab\n",
        "        self.train_iter = train_iter\n",
        "        self.valid_iter = valid_iter\n",
        "        self.skipgrams = skipgrams\n",
        "        self.params = params\n",
        "\n",
        "        self.epoch_train_mins = {}\n",
        "        self.loss = {\"train\": [], \"valid\": []}\n",
        "\n",
        "        # sending all to device\n",
        "        self.model.to(self.params.DEVICE)\n",
        "        self.params.CRITERION.to(self.params.DEVICE)\n",
        "\n",
        "        self.negative_sampler = NegativeSampler(\n",
        "            vocab=self.vocab, ns_exponent=.75,\n",
        "            ns_array_len=self.params.NS_ARRAY_LEN\n",
        "            )\n",
        "        self.testwords = ['pe']\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        self.test_testwords()\n",
        "        for epoch in range(self.params.N_EPOCHS):\n",
        "            # Generate Dataloaders\n",
        "            self.train_dataloader = DataLoader(\n",
        "                self.train_iter,\n",
        "                batch_size=self.params.BATCH_SIZE,\n",
        "                shuffle=False,\n",
        "                collate_fn=self.skipgrams.collate_skipgram\n",
        "            )\n",
        "            self.valid_dataloader = DataLoader(\n",
        "                self.valid_iter,\n",
        "                batch_size=self.params.BATCH_SIZE,\n",
        "                shuffle=False,\n",
        "                collate_fn=self.skipgrams.collate_skipgram\n",
        "            )\n",
        "            # training the model\n",
        "            st_time = monotonic()\n",
        "            self._train_epoch()\n",
        "            self.epoch_train_mins[epoch] = round((monotonic()-st_time)/60, 1)\n",
        "\n",
        "            # validating the model\n",
        "            self._validate_epoch()\n",
        "            print(f\"\"\"Epoch: {epoch+1}/{self.params.N_EPOCHS}\\n\"\"\",\n",
        "            f\"\"\"    Train Loss: {self.loss['train'][-1]:.2}\\n\"\"\",\n",
        "            f\"\"\"    Valid Loss: {self.loss['valid'][-1]:.2}\\n\"\"\",\n",
        "            f\"\"\"    Training Time (mins): {self.epoch_train_mins.get(epoch)}\"\"\"\n",
        "            \"\"\"\\n\"\"\"\n",
        "            )\n",
        "            self.test_testwords()\n",
        "\n",
        "\n",
        "    def _train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = []\n",
        "\n",
        "        for i, batch_data in enumerate(self.train_dataloader, 1):\n",
        "            if len(batch_data[0]) == 0:\n",
        "                continue\n",
        "            inputs = batch_data[0].to(self.params.DEVICE)\n",
        "            pos_labels = batch_data[1].to(self.params.DEVICE)\n",
        "            neg_labels = self.negative_sampler.sample(\n",
        "                pos_labels.shape[0], self.params.NEG_SAMPLES\n",
        "                )\n",
        "            neg_labels = neg_labels.to(self.params.DEVICE)\n",
        "            context = torch.cat(\n",
        "                [pos_labels.view(pos_labels.shape[0], 1),\n",
        "                neg_labels], dim=1\n",
        "              )\n",
        "\n",
        "            # building the targets tensor\n",
        "            y_pos = torch.ones((pos_labels.shape[0], 1))\n",
        "            y_neg = torch.zeros((neg_labels.shape[0], neg_labels.shape[1]))\n",
        "            y = torch.cat([y_pos, y_neg], dim=1).to(self.params.DEVICE)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(inputs, context)\n",
        "            loss = self.params.CRITERION(outputs, y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "        epoch_loss = np.mean(running_loss)\n",
        "\n",
        "        self.loss['train'].append(epoch_loss)\n",
        "\n",
        "    def _validate_epoch(self):\n",
        "        self.model.eval()\n",
        "        running_loss = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch_data in enumerate(self.valid_dataloader, 1):\n",
        "                if len(batch_data[0]) == 0:\n",
        "                    continue\n",
        "                inputs = batch_data[0].to(self.params.DEVICE)\n",
        "                pos_labels = batch_data[1].to(self.params.DEVICE)\n",
        "                neg_labels = self.negative_sampler.sample(\n",
        "                    pos_labels.shape[0], self.params.NEG_SAMPLES\n",
        "                    ).to(self.params.DEVICE)\n",
        "                context = torch.cat(\n",
        "                    [pos_labels.view(pos_labels.shape[0], 1),\n",
        "                    neg_labels], dim=1\n",
        "                  )\n",
        "\n",
        "\n",
        "                # building the targets tensor\n",
        "                y_pos = torch.ones((pos_labels.shape[0], 1))\n",
        "                y_neg = torch.zeros((neg_labels.shape[0], neg_labels.shape[1]))\n",
        "                y = torch.cat([y_pos, y_neg], dim=1).to(self.params.DEVICE)\n",
        "\n",
        "                preds = self.model(inputs, context).to(self.params.DEVICE)\n",
        "                loss = self.params.CRITERION(preds, y)\n",
        "\n",
        "                running_loss.append(loss.item())\n",
        "\n",
        "            epoch_loss = np.mean(running_loss)\n",
        "            self.loss['valid'].append(epoch_loss)\n",
        "\n",
        "    def test_testwords(self, n: int = 5):\n",
        "        for word in self.testwords:\n",
        "            print(word)\n",
        "            nn_words = self.model.get_similar_words(word, n)\n",
        "            for w, sim in nn_words.items():\n",
        "                print(f\"{w} ({sim:.3})\", end=' ')\n",
        "            print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {
        "id": "q6DtwPHfqRVV"
      },
      "outputs": [],
      "source": [
        "params = Word2VecParams()\n",
        "train_iter, valid_iter = get_data()\n",
        "train_iter_context, valid_iter_context = get_data(train_dir='C:/Users/gheto/Desktop/PoS/AromanianPoS/dataset/Tales.train.rup', valid_dir='C:/Users/gheto/Desktop/PoS/AromanianPoS/dataset/Tales.test.rup')\n",
        "tokenizer = get_tokenizer(params.TOKENIZER)\n",
        "vocab_center = build_vocab(train_iter, tokenizer, params)\n",
        "vocab_context = build_vocab(train_iter_context, tokenizer, params)\n",
        "skip_gram = SkipGrams(vocab=vocab_center, vocab_context=vocab_context, params=params, tokenizer=tokenizer)\n",
        "model = Model(vocab=vocab_center, vocab_context=vocab_context, params=params).to(params.DEVICE)\n",
        "optimizer = torch.optim.Adam(params = model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63ZAFXnOxi32",
        "outputId": "c4d18e9a-2e63-4ea4-a690-fc236ce92485"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/8573 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8573/8573 [01:20<00:00, 105.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pe\n",
            "păltări (0.22) calea-calea (0.199) padea (0.197) cu-alantu (0.194) aruca (0.183) \n",
            "\n",
            "Epoch: 1/100\n",
            "     Train Loss: 7.0\n",
            "     Valid Loss: 6.9\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.22) calea-calea (0.199) padea (0.197) cu-alantu (0.194) aruca (0.182) \n",
            "\n",
            "Epoch: 2/100\n",
            "     Train Loss: 7.0\n",
            "     Valid Loss: 7.0\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.22) calea-calea (0.199) padea (0.197) cu-alantu (0.194) aruca (0.182) \n",
            "\n",
            "Epoch: 3/100\n",
            "     Train Loss: 6.9\n",
            "     Valid Loss: 6.9\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.219) calea-calea (0.199) padea (0.197) cu-alantu (0.194) aruca (0.182) \n",
            "\n",
            "Epoch: 4/100\n",
            "     Train Loss: 6.9\n",
            "     Valid Loss: 6.9\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.219) calea-calea (0.199) padea (0.197) cu-alantu (0.194) aruca (0.182) \n",
            "\n",
            "Epoch: 5/100\n",
            "     Train Loss: 6.9\n",
            "     Valid Loss: 6.9\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.219) calea-calea (0.199) padea (0.197) cu-alantu (0.193) aruca (0.182) \n",
            "\n",
            "Epoch: 6/100\n",
            "     Train Loss: 6.8\n",
            "     Valid Loss: 6.9\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.219) calea-calea (0.199) padea (0.197) cu-alantu (0.193) aruca (0.182) \n",
            "\n",
            "Epoch: 7/100\n",
            "     Train Loss: 7.0\n",
            "     Valid Loss: 6.8\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.219) calea-calea (0.199) padea (0.197) cu-alantu (0.193) aruca (0.182) \n",
            "\n",
            "Epoch: 8/100\n",
            "     Train Loss: 6.7\n",
            "     Valid Loss: 6.9\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.219) calea-calea (0.199) padea (0.197) cu-alantu (0.193) aruca (0.182) \n",
            "\n",
            "Epoch: 9/100\n",
            "     Train Loss: 6.7\n",
            "     Valid Loss: 6.9\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.218) calea-calea (0.199) padea (0.197) cu-alantu (0.193) aruca (0.182) \n",
            "\n",
            "Epoch: 10/100\n",
            "     Train Loss: 6.7\n",
            "     Valid Loss: 6.7\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.218) calea-calea (0.199) padea (0.197) cu-alantu (0.193) aruca (0.182) \n",
            "\n",
            "Epoch: 11/100\n",
            "     Train Loss: 6.5\n",
            "     Valid Loss: 6.9\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.218) calea-calea (0.199) padea (0.197) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 12/100\n",
            "     Train Loss: 6.6\n",
            "     Valid Loss: 6.8\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.218) calea-calea (0.199) padea (0.197) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 13/100\n",
            "     Train Loss: 6.6\n",
            "     Valid Loss: 6.8\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.218) calea-calea (0.199) padea (0.197) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 14/100\n",
            "     Train Loss: 6.6\n",
            "     Valid Loss: 6.8\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.218) calea-calea (0.199) padea (0.197) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 15/100\n",
            "     Train Loss: 6.5\n",
            "     Valid Loss: 6.9\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.218) calea-calea (0.199) padea (0.197) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 16/100\n",
            "     Train Loss: 6.3\n",
            "     Valid Loss: 6.7\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.218) calea-calea (0.199) padea (0.197) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 17/100\n",
            "     Train Loss: 6.5\n",
            "     Valid Loss: 6.8\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.218) calea-calea (0.199) padea (0.197) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 18/100\n",
            "     Train Loss: 6.4\n",
            "     Valid Loss: 6.7\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.217) calea-calea (0.199) padea (0.197) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 19/100\n",
            "     Train Loss: 6.4\n",
            "     Valid Loss: 6.7\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.217) calea-calea (0.199) padea (0.197) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 20/100\n",
            "     Train Loss: 6.3\n",
            "     Valid Loss: 6.6\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.217) calea-calea (0.198) padea (0.197) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 21/100\n",
            "     Train Loss: 6.4\n",
            "     Valid Loss: 6.6\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.217) calea-calea (0.198) padea (0.197) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 22/100\n",
            "     Train Loss: 6.3\n",
            "     Valid Loss: 6.8\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.217) calea-calea (0.198) padea (0.197) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 23/100\n",
            "     Train Loss: 6.2\n",
            "     Valid Loss: 6.7\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.216) calea-calea (0.198) padea (0.196) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 24/100\n",
            "     Train Loss: 6.3\n",
            "     Valid Loss: 6.6\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.216) calea-calea (0.198) padea (0.196) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 25/100\n",
            "     Train Loss: 6.3\n",
            "     Valid Loss: 6.7\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.216) calea-calea (0.198) padea (0.196) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 26/100\n",
            "     Train Loss: 6.2\n",
            "     Valid Loss: 6.6\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.216) calea-calea (0.198) padea (0.196) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 27/100\n",
            "     Train Loss: 6.3\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.216) calea-calea (0.198) padea (0.196) cu-alantu (0.192) aruca (0.182) \n",
            "\n",
            "Epoch: 28/100\n",
            "     Train Loss: 6.1\n",
            "     Valid Loss: 6.7\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.216) calea-calea (0.198) padea (0.196) cu-alantu (0.191) aruca (0.182) \n",
            "\n",
            "Epoch: 29/100\n",
            "     Train Loss: 6.0\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.216) calea-calea (0.198) padea (0.196) cu-alantu (0.191) aruca (0.182) \n",
            "\n",
            "Epoch: 30/100\n",
            "     Train Loss: 6.2\n",
            "     Valid Loss: 6.7\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.216) calea-calea (0.198) padea (0.196) cu-alantu (0.191) aruca (0.182) \n",
            "\n",
            "Epoch: 31/100\n",
            "     Train Loss: 6.0\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.216) calea-calea (0.198) padea (0.196) cu-alantu (0.191) aruca (0.182) \n",
            "\n",
            "Epoch: 32/100\n",
            "     Train Loss: 6.1\n",
            "     Valid Loss: 6.6\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.216) calea-calea (0.198) padea (0.196) cu-alantu (0.191) aruca (0.182) \n",
            "\n",
            "Epoch: 33/100\n",
            "     Train Loss: 6.0\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.216) calea-calea (0.198) padea (0.196) cu-alantu (0.191) aruca (0.182) \n",
            "\n",
            "Epoch: 34/100\n",
            "     Train Loss: 6.0\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.216) calea-calea (0.198) padea (0.196) cu-alantu (0.191) aruca (0.181) \n",
            "\n",
            "Epoch: 35/100\n",
            "     Train Loss: 6.0\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.215) calea-calea (0.198) padea (0.196) cu-alantu (0.191) aruca (0.181) \n",
            "\n",
            "Epoch: 36/100\n",
            "     Train Loss: 5.9\n",
            "     Valid Loss: 6.6\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.215) calea-calea (0.198) padea (0.196) cu-alantu (0.191) aruca (0.181) \n",
            "\n",
            "Epoch: 37/100\n",
            "     Train Loss: 5.9\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.215) calea-calea (0.198) padea (0.196) cu-alantu (0.191) aruca (0.181) \n",
            "\n",
            "Epoch: 38/100\n",
            "     Train Loss: 6.0\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.215) calea-calea (0.198) padea (0.196) cu-alantu (0.191) aruca (0.181) \n",
            "\n",
            "Epoch: 39/100\n",
            "     Train Loss: 5.9\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.214) calea-calea (0.198) padea (0.196) cu-alantu (0.191) aruca (0.181) \n",
            "\n",
            "Epoch: 40/100\n",
            "     Train Loss: 5.9\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.214) calea-calea (0.198) padea (0.196) cu-alantu (0.191) aruca (0.181) \n",
            "\n",
            "Epoch: 41/100\n",
            "     Train Loss: 5.9\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.214) calea-calea (0.198) padea (0.196) cu-alantu (0.191) aruca (0.181) \n",
            "\n",
            "Epoch: 42/100\n",
            "     Train Loss: 5.7\n",
            "     Valid Loss: 6.6\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.214) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.181) \n",
            "\n",
            "Epoch: 43/100\n",
            "     Train Loss: 5.8\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.214) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.181) \n",
            "\n",
            "Epoch: 44/100\n",
            "     Train Loss: 5.7\n",
            "     Valid Loss: 6.4\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.214) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.181) \n",
            "\n",
            "Epoch: 45/100\n",
            "     Train Loss: 5.8\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.213) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.181) \n",
            "\n",
            "Epoch: 46/100\n",
            "     Train Loss: 5.7\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.213) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.181) \n",
            "\n",
            "Epoch: 47/100\n",
            "     Train Loss: 5.7\n",
            "     Valid Loss: 6.4\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.213) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.181) \n",
            "\n",
            "Epoch: 48/100\n",
            "     Train Loss: 5.6\n",
            "     Valid Loss: 6.4\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.213) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.181) \n",
            "\n",
            "Epoch: 49/100\n",
            "     Train Loss: 5.7\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.213) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.18) \n",
            "\n",
            "Epoch: 50/100\n",
            "     Train Loss: 5.7\n",
            "     Valid Loss: 6.4\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.213) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.18) \n",
            "\n",
            "Epoch: 51/100\n",
            "     Train Loss: 5.6\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.213) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.18) \n",
            "\n",
            "Epoch: 52/100\n",
            "     Train Loss: 5.5\n",
            "     Valid Loss: 6.3\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.213) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.18) \n",
            "\n",
            "Epoch: 53/100\n",
            "     Train Loss: 5.5\n",
            "     Valid Loss: 6.4\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.212) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.18) \n",
            "\n",
            "Epoch: 54/100\n",
            "     Train Loss: 5.6\n",
            "     Valid Loss: 6.5\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.212) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.18) \n",
            "\n",
            "Epoch: 55/100\n",
            "     Train Loss: 5.6\n",
            "     Valid Loss: 6.4\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.212) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.18) \n",
            "\n",
            "Epoch: 56/100\n",
            "     Train Loss: 5.5\n",
            "     Valid Loss: 6.3\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.212) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.18) \n",
            "\n",
            "Epoch: 57/100\n",
            "     Train Loss: 5.5\n",
            "     Valid Loss: 6.4\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.212) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.18) \n",
            "\n",
            "Epoch: 58/100\n",
            "     Train Loss: 5.6\n",
            "     Valid Loss: 6.4\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.212) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.18) \n",
            "\n",
            "Epoch: 59/100\n",
            "     Train Loss: 5.5\n",
            "     Valid Loss: 6.4\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.212) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.18) \n",
            "\n",
            "Epoch: 60/100\n",
            "     Train Loss: 5.4\n",
            "     Valid Loss: 6.4\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.212) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.18) \n",
            "\n",
            "Epoch: 61/100\n",
            "     Train Loss: 5.5\n",
            "     Valid Loss: 6.4\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.212) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.18) \n",
            "\n",
            "Epoch: 62/100\n",
            "     Train Loss: 5.4\n",
            "     Valid Loss: 6.4\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.211) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.18) \n",
            "\n",
            "Epoch: 63/100\n",
            "     Train Loss: 5.4\n",
            "     Valid Loss: 6.4\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.211) calea-calea (0.198) padea (0.196) cu-alantu (0.19) aruca (0.18) \n",
            "\n",
            "Epoch: 64/100\n",
            "     Train Loss: 5.4\n",
            "     Valid Loss: 6.4\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.211) calea-calea (0.198) padea (0.196) cu-alantu (0.189) aruca (0.18) \n",
            "\n",
            "Epoch: 65/100\n",
            "     Train Loss: 5.3\n",
            "     Valid Loss: 6.4\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.211) calea-calea (0.198) padea (0.196) cu-alantu (0.189) aruca (0.18) \n",
            "\n",
            "Epoch: 66/100\n",
            "     Train Loss: 5.3\n",
            "     Valid Loss: 6.4\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.211) calea-calea (0.198) padea (0.196) cu-alantu (0.189) aruca (0.18) \n",
            "\n",
            "Epoch: 67/100\n",
            "     Train Loss: 5.3\n",
            "     Valid Loss: 6.3\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.211) calea-calea (0.198) padea (0.196) cu-alantu (0.189) aruca (0.179) \n",
            "\n",
            "Epoch: 68/100\n",
            "     Train Loss: 5.3\n",
            "     Valid Loss: 6.3\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.21) calea-calea (0.198) padea (0.196) cu-alantu (0.189) aruca (0.179) \n",
            "\n",
            "Epoch: 69/100\n",
            "     Train Loss: 5.3\n",
            "     Valid Loss: 6.3\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.21) calea-calea (0.198) padea (0.196) cu-alantu (0.189) aruca (0.179) \n",
            "\n",
            "Epoch: 70/100\n",
            "     Train Loss: 5.2\n",
            "     Valid Loss: 6.3\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.21) calea-calea (0.198) padea (0.196) cu-alantu (0.188) aruca (0.179) \n",
            "\n",
            "Epoch: 71/100\n",
            "     Train Loss: 5.3\n",
            "     Valid Loss: 6.3\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.21) calea-calea (0.198) padea (0.196) cu-alantu (0.188) aruca (0.18) \n",
            "\n",
            "Epoch: 72/100\n",
            "     Train Loss: 5.2\n",
            "     Valid Loss: 6.3\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.21) calea-calea (0.198) padea (0.196) cu-alantu (0.187) aruca (0.18) \n",
            "\n",
            "Epoch: 73/100\n",
            "     Train Loss: 5.3\n",
            "     Valid Loss: 6.3\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.21) calea-calea (0.198) padea (0.196) cu-alantu (0.187) aruca (0.18) \n",
            "\n",
            "Epoch: 74/100\n",
            "     Train Loss: 5.2\n",
            "     Valid Loss: 6.3\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.21) calea-calea (0.198) padea (0.196) cu-alantu (0.187) aruca (0.18) \n",
            "\n",
            "Epoch: 75/100\n",
            "     Train Loss: 5.2\n",
            "     Valid Loss: 6.2\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.21) calea-calea (0.198) padea (0.196) cu-alantu (0.187) aruca (0.18) \n",
            "\n",
            "Epoch: 76/100\n",
            "     Train Loss: 5.3\n",
            "     Valid Loss: 6.2\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.21) calea-calea (0.198) padea (0.195) cu-alantu (0.187) aruca (0.18) \n",
            "\n",
            "Epoch: 77/100\n",
            "     Train Loss: 5.2\n",
            "     Valid Loss: 6.3\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.21) calea-calea (0.198) padea (0.195) cu-alantu (0.187) aruca (0.18) \n",
            "\n",
            "Epoch: 78/100\n",
            "     Train Loss: 5.1\n",
            "     Valid Loss: 6.2\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.21) calea-calea (0.198) padea (0.195) cu-alantu (0.186) aruca (0.179) \n",
            "\n",
            "Epoch: 79/100\n",
            "     Train Loss: 5.1\n",
            "     Valid Loss: 6.3\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.21) calea-calea (0.198) padea (0.195) cu-alantu (0.186) aruca (0.179) \n",
            "\n",
            "Epoch: 80/100\n",
            "     Train Loss: 5.2\n",
            "     Valid Loss: 6.2\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.21) calea-calea (0.198) padea (0.195) cu-alantu (0.186) aruca (0.179) \n",
            "\n",
            "Epoch: 81/100\n",
            "     Train Loss: 5.1\n",
            "     Valid Loss: 6.2\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.21) calea-calea (0.198) padea (0.195) cu-alantu (0.186) aruca (0.179) \n",
            "\n",
            "Epoch: 82/100\n",
            "     Train Loss: 5.0\n",
            "     Valid Loss: 6.2\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.21) calea-calea (0.198) padea (0.195) cu-alantu (0.186) aruca (0.179) \n",
            "\n",
            "Epoch: 83/100\n",
            "     Train Loss: 5.0\n",
            "     Valid Loss: 6.2\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.209) calea-calea (0.198) padea (0.195) cu-alantu (0.186) aruca (0.179) \n",
            "\n",
            "Epoch: 84/100\n",
            "     Train Loss: 5.0\n",
            "     Valid Loss: 6.3\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.209) calea-calea (0.198) padea (0.195) cu-alantu (0.186) aruca (0.179) \n",
            "\n",
            "Epoch: 85/100\n",
            "     Train Loss: 5.1\n",
            "     Valid Loss: 6.3\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.209) calea-calea (0.198) padea (0.195) cu-alantu (0.186) aruca (0.179) \n",
            "\n",
            "Epoch: 86/100\n",
            "     Train Loss: 5.1\n",
            "     Valid Loss: 6.2\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.209) calea-calea (0.198) padea (0.195) cu-alantu (0.186) aruca (0.179) \n",
            "\n",
            "Epoch: 87/100\n",
            "     Train Loss: 4.8\n",
            "     Valid Loss: 6.2\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.209) calea-calea (0.199) padea (0.195) cu-alantu (0.186) aruca (0.179) \n",
            "\n",
            "Epoch: 88/100\n",
            "     Train Loss: 5.1\n",
            "     Valid Loss: 6.1\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.209) calea-calea (0.199) padea (0.195) cu-alantu (0.186) aruca (0.179) \n",
            "\n",
            "Epoch: 89/100\n",
            "     Train Loss: 5.0\n",
            "     Valid Loss: 6.2\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.209) calea-calea (0.199) padea (0.195) cu-alantu (0.186) aruca (0.179) \n",
            "\n",
            "Epoch: 90/100\n",
            "     Train Loss: 5.0\n",
            "     Valid Loss: 6.3\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.208) calea-calea (0.199) padea (0.194) cu-alantu (0.186) aruca (0.179) \n",
            "\n",
            "Epoch: 91/100\n",
            "     Train Loss: 4.9\n",
            "     Valid Loss: 6.2\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.208) calea-calea (0.199) padea (0.194) cu-alantu (0.185) aruca (0.179) \n",
            "\n",
            "Epoch: 92/100\n",
            "     Train Loss: 5.0\n",
            "     Valid Loss: 6.2\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.208) calea-calea (0.199) padea (0.194) cu-alantu (0.185) aruca (0.179) \n",
            "\n",
            "Epoch: 93/100\n",
            "     Train Loss: 5.0\n",
            "     Valid Loss: 6.1\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.208) calea-calea (0.199) padea (0.194) cu-alantu (0.185) aruca (0.179) \n",
            "\n",
            "Epoch: 94/100\n",
            "     Train Loss: 4.9\n",
            "     Valid Loss: 6.2\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.208) calea-calea (0.199) padea (0.194) cu-alantu (0.185) pre-anarga (0.179) \n",
            "\n",
            "Epoch: 95/100\n",
            "     Train Loss: 4.9\n",
            "     Valid Loss: 6.2\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.208) calea-calea (0.199) padea (0.194) cu-alantu (0.185) pre-anarga (0.179) \n",
            "\n",
            "Epoch: 96/100\n",
            "     Train Loss: 4.9\n",
            "     Valid Loss: 6.2\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.208) calea-calea (0.199) padea (0.194) cu-alantu (0.185) pre-anarga (0.179) \n",
            "\n",
            "Epoch: 97/100\n",
            "     Train Loss: 4.9\n",
            "     Valid Loss: 6.1\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.208) calea-calea (0.199) padea (0.194) cu-alantu (0.185) pre-anarga (0.179) \n",
            "\n",
            "Epoch: 98/100\n",
            "     Train Loss: 4.8\n",
            "     Valid Loss: 6.1\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.207) calea-calea (0.199) padea (0.194) cu-alantu (0.185) pre-anarga (0.179) \n",
            "\n",
            "Epoch: 99/100\n",
            "     Train Loss: 4.9\n",
            "     Valid Loss: 6.1\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.207) calea-calea (0.199) padea (0.194) cu-alantu (0.186) pre-anarga (0.179) \n",
            "\n",
            "Epoch: 100/100\n",
            "     Train Loss: 4.9\n",
            "     Valid Loss: 6.1\n",
            "     Training Time (mins): 0.0\n",
            "\n",
            "pe\n",
            "păltări (0.207) calea-calea (0.199) padea (0.194) cu-alantu (0.186) pre-anarga (0.179) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        params=params,\n",
        "        optimizer=optimizer,\n",
        "        train_iter=train_iter,\n",
        "        valid_iter=valid_iter,\n",
        "        vocab=vocab_context, # vocabularul de aici e folosit pentru negative sampling, pe care il facem din context\n",
        "        skipgrams=skip_gram\n",
        "    )\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pe\n",
            "păltări (0.207) calea-calea (0.199) padea (0.194) cu-alantu (0.186) pre-anarga (0.179) \n",
            "\n",
            "douăzeci\n",
            "nicaț (0.226) adunai (0.199) 1906 (0.194) cuibul (0.18) asvindzeam (0.18) \n",
            "\n",
            "mănâncă\n",
            "fapțîl’i (0.213) chetrile (0.198) tatălui (0.198) aistă (0.197) aușatic (0.197) \n",
            "\n",
            "penele\n",
            "armân (0.209) topcă (0.198) arșițile (0.192) pitrumsiră (0.191) aprindeț (0.188) \n",
            "\n",
            "băiete\n",
            "daț-le (0.208) tu-apirită (0.203) fumăria (0.192) asculți (0.188) harfă (0.188) \n",
            "\n",
            "furtuna\n",
            "se-andrupă (0.202) suflite (0.188) s-filipsească (0.187) singură (0.182) ficior-fic (0.176) \n",
            "\n",
            "zbura\n",
            "bag-u (0.195) di-aclo (0.189) armănea (0.188) arse (0.182) dorină (0.177) \n",
            "\n",
            "citind\n",
            "nîs“ (0.206) piricl’iu (0.204) dusiră (0.184) graiurî (0.18) juneaște (0.179) \n",
            "\n",
            "armânii\n",
            "bîna (0.197) nîs (0.195) acățață (0.186) dări (0.184) hapse (0.179) \n",
            "\n",
            "oile\n",
            "ancl’igat (0.211) ponda (0.202) pri-aclo (0.187) altă-oară (0.184) mul’erle (0.176) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "def test_testwords(self, n: int = 5):\n",
        "    for word in [\"pe\", \"douăzeci\", \"mănâncă\", \"penele\", \"băiete\", \"furtuna\", \"zbura\", \"citind\", \"armânii\", \"oile\"]:\n",
        "        print(word)\n",
        "        nn_words = self.model.get_similar_words(word, n)\n",
        "        for w, sim in nn_words.items():\n",
        "            print(f\"{w} ({sim:.3})\", end=' ')\n",
        "        print('\\n') \n",
        "\n",
        "test_testwords(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pe\n",
            "chitroasă (0.213) vrură (0.201) lire (0.2) agioclu (0.196) hearele (0.194) \n",
            "\n",
            "douăzeci\n",
            "turma (0.224) stole (0.199) guli (0.196) chiñi (0.191) s-le-aibă (0.187) \n",
            "\n",
            "mănâncă\n",
            "mire (0.211) avhil’eate (0.207) s-minduia (0.19) l’epur (0.188) de-asime (0.187) \n",
            "\n",
            "penele\n",
            "ciuciteaște (0.218) lu-agiungu (0.201) ambar (0.198) alăgară (0.197) stres-stres (0.187) \n",
            "\n",
            "băiete\n",
            "zulăchilor (0.232) vilendză (0.23) mîcate (0.224) iu-ț (0.203) plăteaște (0.197) \n",
            "\n",
            "furtuna\n",
            "cutie (0.249) bunu (0.202) mîcară (0.194) n-afla (0.191) tăl’eat (0.183) \n",
            "\n",
            "zbura\n",
            "s-cutrimburară (0.225) s-γină“ (0.209) s-aungă (0.209) salți (0.199) angreacă (0.197) \n",
            "\n",
            "citind\n",
            "bărbărută (0.232) alîndurle (0.216) cot (0.211) tălăgane (0.209) cuțute (0.208) \n",
            "\n",
            "armânii\n",
            "a-nveastil’ei (0.191) căftară (0.187) avea-ntunicată (0.185) cărave (0.183) stihio (0.172) \n",
            "\n",
            "oile\n",
            "fufulii (0.248) paradhis (0.198) mîrînγipsite (0.188) bucură-te (0.182) dizlichi (0.181) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "def test_testwords(self, n: int = 5):\n",
        "    for word in [\"pe\", \"douăzeci\", \"mănâncă\", \"penele\", \"băiete\", \"furtuna\", \"zbura\", \"citind\", \"armânii\", \"oile\"]:\n",
        "        print(word)\n",
        "        nn_words = self.model.get_similar_words(word, n)\n",
        "        for w, sim in nn_words.items():\n",
        "            print(f\"{w} ({sim:.3})\", end=' ')\n",
        "        print('\\n') \n",
        "\n",
        "test_testwords(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Doar niste experimente\n",
        "text_tokens = vocab_center.get_index(tokenizer(\"Sunt unele deprinderi pe care le câștigi doar citind, ori vorbindu-ți-se despre ele. Dar mai sunt și din cele ce nu ne intră în cap, până nu le vedem cu ochii. Cât trăim - multe auzim și multe vedem, dar cu toate astea - nimic nu vom ști ca lumea până nu punem mâna să și facem ceea ce ne pare că știm. Cu alte cuvinte, vreau să spun eu, teoria e bună ea, deseori, dar câte o dată rămâne de căruță față de practică. Așa grăia într-un rând un bătrân înțelept - către niște tineri ce încă nu ieșiseră din școală, și care își făceau ideea cum că... nimeni nu-i mai învățat ca ei, și că... tot ce zboară, se mănâncă! Ca să pricepeți ce vă spusei până aici, luați\"))\n",
        "print(text_tokens)\n",
        "context_words = []\n",
        "context_tokens = []\n",
        "for text in text_tokens:\n",
        "    context_word = vocab_context.lookup_token(text)\n",
        "    context_words.append(context_word)\n",
        "    context_tokens.append(vocab_context.get_index(context_word))\n",
        "print(context_tokens)\n",
        "print(context_words)\n",
        "\n",
        "print(tokenizer(\"TEORIA SI PRACTICA \\nSunt unele deprinderi pe care le câștigi doar citind, ori vorbindu-ți-se despre ele.\"))\n",
        "print(tokenizer(\"TEORIA SI PRACTICA \\nSănt îndoauă învețuri, cari si amintă mași cu ghivăsirea, i cu avdzărea.\"))\n",
        "\n",
        "print(vocab_center.get_index(\"sunt\"))\n",
        "print(vocab_context.get_index(\"sănt\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
